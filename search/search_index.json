{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Leettools","text":"<p>visit leettools repo at Leetools.</p> <p>LeetTools is an AI search assistant that can perform highly customizable search workflows and save the search results and generated outputs to local knowledge bases. With an automated document pipeline that handles data ingestion, indexing, and storage, we can easily run complext search workflows that query, extract and generate content from the web or local knowledge bases.</p> <p>LeetTools can run with minimal resource requirements on the command line with a DuckDB-backend and configurable LLM settings. It can be easily integrated with other applications need AI search and knowledge base support.</p>"},{"location":"documentation/","title":"LeetTools","text":""},{"location":"documentation/#problems","title":"Problems","text":"<p>LLM-based search applications such as Perplexity and ChatGPT search have become  increasingly popular recently. However, instead of simple question-answering interactions,  sometimes we need to perform more complex search-based tasks that need iterative workflows, personalized data curation, and domain-specific knowledge integration. </p> <p>```Markdown title=\"Example: Search and Summarize\" When we do a web search, since the content we need may not always be available on the  first page of search results, can we go a few more pages to find only relevant documents and then summarize the relevant information? For such a search workflow, we can: 1. Use a search engine to fetch the top documents, up to X pages. 2. Crawl the result URLs to fetch the content. 3. Use LLM to summarize the content of each page to see if the content is relevant. 4. We can also crawl links found in the content to fetch more relevant information. 5. When we reach a predefined threshold, say number of relevant documents, or number of    iterations, we can stop the search. 6. Aggregate all the relevant summaries to generate a list of topics discussed in the    search results. 7. Use the topics to generate a digest article that summarizes the search results.</p> <pre><code>\nHere are a few more examples:\n- Simple:\n    - Search and summarize: search for a topic, in the search results, go through the top\n        X instead of only the top 10, filter out the unrelated ones, generate a digest \n        article from the search results.\n    - Customized search: Can I limit my search to a specific domain or source or a date range? \n        Can I query in language X, search in language Y, and generate the answer in language Z?\n        Can I exclude the search results from a specific source? Can I generate the results\n        in a specific style with a specific length?\n- Complex:\n    - Extract and dedupe: find all entities that satisfy a condition, extract required\n        information as structured data, and deduplicate the results.\n    - Search and aggregate: given a product, search for all recently reviews and feedbacks,\n        identify the sentiment, the product aspects, and the suggestions, aggregate the\n        results based on the sentiment and the aspects.\n- Hard:\n    - Dynamic streaming queries: monitor the web for a specific topic, find the \"new\" and \"hot\"\n        information that I have not seen before and satisfies a certain criteria, summarize\n        or extract the content I need and generate a report in my customized format.\n\nAfter analyzing why it is hard to implement such tasks, we found that the main reason\nis the lack of data support for the iterative workflows. Therefore, we want to make \na persistent data layer to support the complex logic required for such tasks.\n\n\n## Solution: search flow with a document pipeline\nLeetTools enables implementation of automated search flows backed by a local document\npipeline. Besides the common benefits of a private deployment such as security and using\nlocal models, LeetTools provides many benefits:\n\n- integrated pipeline to abstract away the complex setup for all the components;\n- easier to implement customized/automated/complex search-task logic;\n- better control over the indexing, retrieval, and ranking process;\n- allow personalized data curations and annotations process;\n- more flexible to adapt models and functionalities suitable for the requirements.\n\n\n## Key Components\n\n![LeetTools Document Pipeline](https://gist.githubusercontent.com/pengfeng/4b2e36bda389e0a3c338b5c42b5d09c1/raw/6bc06db40dadf995212270d914b46281bf7edae9/leettools-eds-arch.svg)\n\nLeetTools provides the following key components:\n\n- A document pipeline to ingest, convert, chunk, embed, and index documents. User \n  specifies a document source such as a search query, a local directory, or a single \n  file, the pipeline will ingest the documents specified by the source to a document \n  sink (the original form of the document) and convert the original document into the\n  standard Markdown format document. The Markdown document is then split and indexed \n  using different configurable strategies. The pipeline is similar to the ETL process\n  of the data pipeline, but the target is now documents instead of structured or \n  semi-structured data.\n- A knowledge base to manage and serve the indexed documents, including the documents, \n  the segments, the embeddings, the document graph, the entity graph, which can be \n  supported by different storage plugins. In this version, we provide the DuckDB-based\n  implementations to minimize the resource footprint.\n- A search and retrieval library used by the document pipeline to retrieve documents. \n  We can use search APIs such as Google, Bing, and Tavily, and scraper APIs such as \n  Firecrawl or Crawl4AI. \n- A workflow engine to implement search-based AI workflows, which provides a thin-layer\n  of abstraction to manage the dependencies and configurations.\n- A configuration system to manage the configurations used in the pipeline, such as the\n  different endpoints, different parameters in the retrieval process, and etc.\n- A query history system to manage the history and the context of the queries.\n- A scheduler to schedule the execution of the ingestions. We provide a simple pull-based\n  scheduler that queries the knowledgebase and execute different tasks (ingestion,\n  converting, splitting, indexing) based on the status of the documents. It is possible to\n  schedule the tasks using more sophisticated schedulers, but we provide an integrated\n  simple scheduler to avoid complex setup and unnecessary dependencies for basic workloads.\n- An accounting system to track the usage of the LLM APIs. For all LLM API calls used in\n  the pipeline and workflow, the system records the prompts, the provider, the tokens\n  used, the results returned. The goal is to provide observability to the whole\n  pipeline and foundation for optimization.\n\nAll you need to do to implement a search-based AI tool is to write a single Python script\nthat organizes different components in the LeetTools framework. An example of such a\nscript is shown in \n[src/leettools/flow/flows/answer/flow_answer.py](src/leettools/flow/flows/answer/flow_answer.py), \nwhich implements the search-extract-answer flow similar to the existing AI search services.\n\nSo, if you want to implement personalized search-based workflows that can accumulate\ndomain-specific knowledge with a persistent local memory, but setting up all the \ncomponents from scratch is too much work, LeetTools is the right tool for you.\n\n## Demo Cases\n\nWe list a few demo use cases that are provided in the codebase:\n\n### Answer with references (similar to Perplexity AI)\n\nSearch the web or local KB with the query and answer with source references:\n\n- Perform the search with retriever: \"local\" for local KB, a search engine\n  (e.g., google) fetches top documents from the web. If no KB is specified, \n  create an adhoc KB; otherwise, save and process results in the KB.\n- New web search results are processed by the document pipeline: conversion,\n  chunking, and indexing.\n- Retrieve top matching segments from the KB based on the query.\n- Concatenate the segments to create context for the query.\n- Use the context to answer with source references via an LLM API call.\n\n### Search and Summarize\n\nWhen interested in a topic, you can generate a digest article from the search results:\n\n- Define search keywords and optional content instructions for relevance filtering.\n- Perform the search with retriever: \"local\" for local KB, a search engine (e.g., Google)\n  fetches top documents from the web. If no KB is specified, create an adhoc KB; \n  otherwise, save and process results in the KB.\n- New web search results are processed through the document pipeline: conversion, \n  chunking, and indexing.\n- Each result document is summarized using a LLM API call.\n- Generate a topic plan for the digest from the document summaries.\n- Create sections for each topic in the plan using content from the KB.\n- Concatenate sections into a complete digest article.\n\n### Search and Extract\n\nExtra structured data from web or local KB search results:\n- Perform the search with retriever: \"local\" for local KB, a search engine (e.g., Google)\n  fetches top documents from the web. If no KB is specified, create an adhoc KB; \n  otherwise, save and process results in the KB.\n- New web search results are processed through the document pipeline: conversion, \n  chunking, and indexing.\n- Extract structured data from matched documents based on the specified model.\n- Display the extracted data as a table in the output.\n\n\n### Search and generate with style\n\nYou can generate an article with a specific style from the search results. \n\n- Specify the number of days to search for news (right now only Google search is \n  supported for this option);\n- LeetTools crawls the web with the keywords in the topic and scrape the top documents to\n  the knowledge base;\n- Saved documents are processed through the document pipeline: conversion, chunking, and\n  indexing;\n- The summaries for the documents are provided to the LLM API to generate a news article\n  with the specified style;\n- You can specify the output language, the number of words, and article style.\n\n\n## Command line commands\n\nThen you can run the command line commands, assuming all commands are run under the root\ndirectory of the project. Run the \"leet list\" command to see all the available commands:\n\n```bash\n% leet list\nlist    List all CLI commands and subcommands.\n...\nflow    Run the flow for the query.\n</code></pre>"},{"location":"documentation/#flow-execution","title":"flow execution","text":"<p>You can run any flow using the <code>leet flow</code> command.</p> <pre><code># list all the flows\nleet flow --list\n# check the parameters for a flow\nleet flow -t answer --info\n# run an answer flow with the default settings\nleet flow -t answer -q \"What is GraphRAG\"\n# run an answer flow with extra parameters, such as the days_limit and output_language\nleet flow -t answer -q \"What is GraphRAG\" -p days_limit=3 -p output_language=es\n# run an answer flow and save the output to a KB\nleet flow -t answer -q \"What is GraphRAG\" -k graphrag\n# run an answer flow on the local KB\nleet flow -t answer -q \"What is GraphRAG\" -k graphrag -p retriever_type=local\n# run an digest of search results from the last three days and output in Spanish\n# this query will run for a while to fetch and analyze the search results\nleet flow -t digest -q \"LLM GenAI News\" -k genai -p days_limit=3 -p output_language=es -l info\n# extract the structured data from the search results\nleet flow -t extract -q \"LLM GenAI Startup\" -k genai -p extract_pydantic=docs/company.py -l info\n</code></pre>"},{"location":"Flow/answer/","title":"Answer","text":""},{"location":"Flow/answer/#feature-answer","title":"Feature: Answer","text":"<p>Description:</p> <p>The Answer workflow in LeetTools is designed to provide direct answers to user queries, enriched with source references. It searches through local knowledge bases (KBs) or the web, retrieving the most relevant information to answer the query effectively. With configurable options, users can tailor the answer generation process to their specific needs, including the style of the answer, language, and source filtering.</p>"},{"location":"Flow/answer/#how-it-works","title":"How it Works:","text":"<ol> <li>Query Processing:</li> </ol> <p>The workflow initiates by receiving a query, which can either be targeted at a local knowledge base (KB) or the web (the entire web or specific targeted websites). If a local KB is specified, the system searches the relevant content; otherwise, it pulls the query results from the web via a search engine (e.g., Google).</p> <ol> <li> <p>Retriever Mechanism:</p> </li> <li> <p>Local KB Search: If a local KB is provided, the query is directed towards it, using the specified <code>docsource_uuid</code> for targeted searches.</p> </li> <li>Web Search: When no KB is specified, the workflow performs a web search to fetch top documents that are most likely to contain relevant information for the query. The search behavior can be further customized with parameters like <code>retriever_type</code> (e.g., Google) and <code>excluded_sites</code> (to avoid specific domains).</li> <li> <p>Fallback Search: If web search yields no relevant results, the system automatically falls back to searching specified KBs. This ensures comprehensive coverage by leveraging both web and local knowledge sources for optimal results.</p> </li> <li> <p>Document Pipeline:</p> </li> </ol> <p>The fetched search results are processed through the document pipeline, which includes:</p> <ul> <li>Conversion: Raw documents are transformed into a structured format.</li> <li>Chunking: Large documents are divided into smaller, manageable segments.</li> <li> <p>Indexing: These segments are indexed for quick retrieval.</p> </li> <li> <p>Contextual Retrieval:</p> </li> </ul> <p>Based on the user query, the workflow retrieves the most relevant segments from the indexed KB or web results. These segments are then concatenated to create a cohesive context, providing the LLM with detailed background information.</p> <ol> <li>Answer Generation:</li> </ol> <p>Using the context formed from the retrieved segments, the LLM generates an answer to the query. The answer is generated with high accuracy and is accompanied by source references, ensuring transparency and credibility. Additional parameters enhance the answer customization:</p> <ul> <li>Article Style (<code>article_style</code>): Choose from various output styles, such as analytical research reports, humorous news articles, or technical blog posts. The default style is analytical research reports.</li> <li>Word Count (<code>word_count</code>): Control the length of the generated answer, or leave it empty for automatic word count adjustment.</li> <li>Language (<code>output_language</code>): Specify the language in which the result should be output.</li> <li>Reference Style (<code>reference_style</code>): Select the reference style, such as default or news, for the citations in the answer.</li> <li>Strict Context (<code>strict_context</code>): Determine whether the LLM should strictly adhere to the context when generating the answer.</li> <li> <p>Example Output (<code>output_example</code>): Provide an example of the expected output, guiding the LLM to align with the desired response style.</p> </li> <li> <p>Search Customization:</p> </li> </ul> <p>The web search process can be finely tuned with the following options:</p> <ul> <li>Search Iteration (<code>search_iteration</code>): If the initial results do not meet expectations, you can define how many additional pages the search should explore.</li> <li>Search Max Results (<code>search_max_results</code>): Limit the number of search results returned by the retriever.</li> <li>Target Site (<code>target_site</code>): Limit the search to a specific website if needed.</li> <li>Days Limit (<code>days_limit</code>): Limit the search to a specified number of days, or leave it empty for no time restriction.</li> </ul>"},{"location":"Flow/answer/#key-benefits","title":"Key Benefits:","text":"<ul> <li>Customization: Fine-tune your search and answer generation with options such as output style, word count, language, and reference format.</li> <li>Efficiency: Automate the search and answer generation process, reducing manual work.</li> <li>Contextual Accuracy: By leveraging contextual retrieval and LLM-based response generation, answers are both precise and comprehensive.</li> <li>Transparency: Source references are provided alongside the answers for verification and reliability.</li> </ul>"},{"location":"Flow/answer/#example-use-case","title":"Example Use Case:","text":"<p>To search for an answer related to \"Quantum Computing\", you can specify:</p> <ul> <li>Article Style as \"technical blog post\"</li> <li>Word Count as 500 words</li> <li>Language as English</li> <li>Excluded Sites like \"example.com\"</li> <li>Target Site as \"news.ycombinator.com\"</li> <li>Knowledge Base as \"quantum_kb\" (if not exists, a new KB will be created to store search results)</li> </ul> <p>These parameters allow you to tailor the results to match your needs while ensuring relevant, high-quality answers.</p> <p>Try this command in your terminal:</p> <pre><code>leet flow -t answer -q \"What is Quantum Computing?\" -k quantum_kb -l info -p article_style=\"technical blog post\" -p word_count=500 -p output_language=\"en\" -p excluded_sites=\"example.com\" -p target_site=\"news.ycombinator.com\" \n</code></pre>"},{"location":"Flow/digest/","title":"Digest","text":""},{"location":"Flow/digest/#feature-digest","title":"Feature: Digest","text":"<p>Description:</p> <p>The Digest workflow in LeetTools is designed to generate a comprehensive, multi-section digest article based on search results. Whether you're researching a topic or compiling relevant information, this feature helps you gather, summarize, and organize content into an easy-to-read digest, all with customizable options to suit your specific needs.</p>"},{"location":"Flow/digest/#how-it-works","title":"How it Works:","text":"<ol> <li>Query and Content Filtering:</li> </ol> <p>The user initiates the workflow by defining a search query. Optionally, the user can provide content instructions (<code>content_instruction</code>) to assess the relevance of the result documents. This helps refine the selection of content to be included in the digest article.</p> <ol> <li> <p>Retriever Mechanism:</p> </li> <li> <p>Local KB Search: If a local knowledge base (KB) is specified (<code>docsource_uuid</code>), the system queries it directly.</p> </li> <li> <p>Web Search: If no KB is provided, the workflow searches the web using a search engine (e.g., Google). Customizations like <code>excluded_sites</code> (to filter out specific domains) or <code>target_site</code> (to restrict searches to a specific website) can be applied.</p> </li> <li> <p>Document Pipeline:</p> </li> </ol> <p>The documents fetched through the search process are then passed through the document pipeline:</p> <ul> <li>Conversion: Raw documents are transformed into a structured format.</li> <li>Chunking: The documents are broken down into smaller, digestible segments.</li> <li> <p>Indexing: These segments are indexed for efficient retrieval.</p> </li> <li> <p>Summarization and Topic Planning:</p> </li> </ul> <p>Each document in the search results is summarized using an LLM API call. A topic plan for the digest article is then generated based on these summaries. The user can specify the number of sections (<code>num_of_sections</code>) for the article or let the planning agent decide automatically.</p> <ol> <li>Section Creation:</li> </ol> <p>The digest article is divided into sections based on the topic plan. Content from the KB or the web is used to fill in each section. The sections are then concatenated into a cohesive article.</p> <ol> <li>Customization Options:</li> </ol> <p>Users can fine-tune the output using the following parameters:</p> <ul> <li>Article Style (<code>article_style</code>): Choose from various output styles, such as analytical research reports, humorous news articles, or technical blog posts.</li> <li>Language (<code>output_language</code>): Output the result in a specific language.</li> <li>Word Count (<code>word_count</code>): Specify the desired word count for each section.</li> <li>Recursive Scraping (<code>recursive_scrape</code>): Enable recursive scraping to gather additional content from URLs found in the search results.</li> <li> <p>Search Customizations: Control search behavior with options like <code>search_max_results</code>, <code>search_iteration</code>, and <code>scrape_max_count</code>.</p> </li> <li> <p>Final Article Generation:</p> </li> </ul> <p>After generating and organizing the sections, the digest article is composed and returned in the specified format, complete with references and citations based on the chosen <code>reference_style</code>.</p>"},{"location":"Flow/digest/#key-benefits","title":"Key Benefits:","text":"<ul> <li>Organized Content: Automatically generates a well-structured, multi-section digest article, perfect for research or content curation.</li> <li>Customization: Fine-tune the article style, word count, search behavior, and more to suit your specific needs.</li> <li>Efficient Research: Combine relevant information from various sources into a cohesive article with minimal manual effort.</li> <li>Transparency: Include references in the output with customizable citation styles, ensuring the credibility of the content.</li> </ul>"},{"location":"Flow/digest/#example-user-case","title":"Example User Case:","text":"<p>To generate a digest article on \"Quantum Computing,\" you can run the following command:</p> <pre><code>leet flow -t digest -q \"What is Quantum Computing?\" -k quantum_kb -l info -p article_style=\"analytical research reports\" -p num_of_sections=5 -p output_language=\"en\" -p recursive_scrape=True -p search_max_results=15 -p search_iteration=2\n</code></pre>"},{"location":"Flow/digest/#explanation","title":"Explanation:","text":"<ul> <li><code>t digest</code>: Specifies the Digest workflow.</li> <li><code>q \"What is Quantum Computing?\"</code>: The search query for the digest article.</li> <li><code>k quantum_kb</code>: Saves the scraped web pages to the knowledge base <code>quantum_kb</code>.</li> <li><code>l info</code>: Sets the log level to <code>info</code>, showing essential log messages.</li> <li><code>p article_style=\"analytical research reports\"</code>: Sets the output style to an analytical research report.</li> <li><code>p num_of_sections=5</code>: Specifies that the digest should have 5 sections.</li> <li><code>p output_language=\"en\"</code>: Outputs the article in English.</li> <li><code>p recursive_scrape=True</code>: Enables recursive scraping of URLs found in the search results.</li> <li><code>p search_max_results=15</code>: Limits the search to a maximum of 15 results.</li> <li><code>p search_iteration=2</code>: Allows the search to go two pages deep for additional results.</li> </ul>"},{"location":"Flow/extract/","title":"Extract","text":""},{"location":"Flow/extract/#feature-extract","title":"Feature: Extract","text":"<p>Description:</p> <p>The Extract workflow in LeetTools allows users to extract structured data from search results\u2014whether they come from a local knowledge base (KB) or the web. After performing the search and processing the results through the document pipeline (conversion, chunking, and indexing), this workflow extracts relevant data points from the matched documents and outputs them in a user-specified format (CSV, JSON, or Markdown). This is ideal for extracting information like company names, product details, investment rounds, and more from web pages or documents in the KB.</p>"},{"location":"Flow/extract/#how-it-works","title":"How it Works:","text":"<ol> <li>KB or Web Search:</li> <li>The workflow starts by performing a search using a retriever.<ul> <li>\"Local\" for querying the KB.</li> <li>A web search engine (e.g., Google) fetches documents if no KB is specified.</li> </ul> </li> <li>If no existing KB is specified, an adhoc KB is created to save and process the results.</li> <li>Document Processing:</li> <li>New web search results are processed through the document pipeline: conversion, chunking, and indexing.</li> <li>This ensures the data is ready for extraction.</li> <li>Data Extraction:</li> <li>Structured data is extracted based on a specified Pydantic model (see documentation for schema creation).</li> <li>The model defines the exact data structure, making it possible to extract details such as product names, dates, locations, and other relevant fields.</li> <li>Output:</li> <li>The extracted data is displayed in a structured format (CSV, JSON, or Markdown) according to the user's specifications.</li> <li>Users can choose the output format with the <code>extract_output_format</code> parameter.</li> <li>Saving Extracted Data:</li> <li>By default, the extracted data is saved to the backend, preserving metadata such as the document\u2019s URI and import time.</li> <li>If this behavior is not desired, users can disable saving by setting <code>extract_save_to_backend=False</code>.</li> <li>Customization Options:    Users can adjust the extraction process with the following parameters:<ul> <li><code>days_limit</code>: Limit the search to results from a specific time range.</li> <li><code>extract_output_format</code>: Choose the output format (e.g., CSV, JSON, or Markdown).</li> <li><code>extract_pydantic</code>: Define the schema for the target data using Pydantic models.</li> <li><code>output_language</code>: Specify the language for the output.</li> <li><code>image_search</code>: Limit the search to image results when searching the web.</li> <li><code>retriever_type</code>: Choose the retriever type (default is Google).</li> <li><code>search_max_results</code>: Set the maximum number of search results to retrieve.</li> </ul> </li> <li>Result Output:</li> <li>The final output is a clean, structured table or file containing the extracted data, ready for analysis or further processing.</li> </ol>"},{"location":"Flow/extract/#key-benefits","title":"Key Benefits:","text":"<ul> <li>Structured Data: Easily extract structured information from unstructured web or KB content.</li> <li>Customizable Output: Choose the output format (CSV, JSON, or Markdown) that best suits your needs.</li> <li>Flexible Search: Use local KB or web search with a variety of retrievers (Google, etc.).</li> <li>Schema-based Extraction: Extract only the data you need by defining the schema with Pydantic models.</li> <li>Data Saving: Optionally save extracted data to the backend with relevant metadata.</li> </ul>"},{"location":"Flow/extract/#example-user-case","title":"Example User Case:","text":"<p>To extract information related to \"AI in Healthcare\" from the web and output the data in CSV format, you can use the following command:</p> <pre><code>% leet flow -t extract -q \"Comapny doing AI in Healthcare\" -k \"AI in Healthcare\" -l info -p days_limit=30 -p extract_output_format=csv -p extract_pydantic=\"docs/company.py\" -p output_language=\"en\"\n\n</code></pre>"},{"location":"Flow/extract/#explanation","title":"Explanation:","text":"<ul> <li><code>t extract</code>: Specifies the Extract workflow.</li> <li><code>q \"Company doing AI in Healthcare\"</code>: The search query to extract relevant data.</li> <li><code>l info</code>: Sets the log level to <code>info</code>, which provides essential log messages.</li> <li><code>p days_limit=30</code>: Limits the search to items published within the last 30 days.</li> <li><code>p extract_output_format=csv</code>: Outputs the extracted data in CSV format.</li> <li><code>p extract_pydantic=\"docs/company.py\"</code>: Specifies the path to the Pydantic model for structured extraction.</li> <li><code>p output_language=\"en\"</code>: Outputs the results in English.</li> </ul> <pre><code>class Company(BaseModel):\n    name: str\n    description: str\n    industry: str\n    main_product_or_service: str\n</code></pre>"},{"location":"Flow/extract/#attributes-fields-of-company","title":"Attributes (Fields) of <code>Company</code>:","text":"<ul> <li><code>name: str</code>: This attribute stores the name of the company, and it is expected to be a string.</li> <li><code>description: str</code>: This attribute stores a description of the company, and it is expected to be a string.</li> <li><code>industry: str</code>: This field represents the industry that the company belongs to (e.g., \"Technology,\" \"Healthcare\"), and it is also expected to be a string.</li> <li><code>main_product_or_service: str</code>: This field represents the main product or service offered by the company (e.g., \"Cloud Computing,\" \"Medical Devices\"), and it should be a string as well.</li> </ul> <p>This workflow is highly useful for data extraction, especially for tasks that require the systematic collection of structured information from large sets of documents or web pages. </p>"},{"location":"Flow/news/","title":"News","text":""},{"location":"Flow/news/#feature-news","title":"Feature: News","text":"<p>Description:</p> <p>The News workflow in LeetTools helps users generate a list of the latest news items related to a specific topic from an updated local knowledge base (KB). This feature focuses on finding, consolidating, and ranking the most relevant news items, ensuring that users are kept up-to-date with the latest information. The workflow performs multiple steps to ensure that duplicate items are removed, and news items are ranked based on the number of sources reporting them.</p>"},{"location":"Flow/news/#how-it-works","title":"How it Works:","text":"<ol> <li>KB Check and Document Retrieval:</li> <li>The workflow checks the KB for the most recently updated documents.</li> <li>It scans these documents to find news items, focusing on newly added or modified content.</li> <li>Combining Similar News Items:</li> <li>News items with similar content are grouped together, reducing redundancy and improving clarity.</li> <li>The grouping ensures that multiple sources reporting the same news are combined into a single news item.</li> <li>Removing Duplicates:</li> <li>Any news items that have already been reported are removed from the results, ensuring that users only receive fresh updates.</li> <li>Ranking by Source Count:</li> <li>The remaining news items are ranked based on the number of sources reporting them. This ensures that the most widely covered news items are given higher priority.</li> <li>Generate News List:</li> <li>A final list of news items is generated, each with references to the sources where the news was reported. This allows users to access the original documents for more detailed information.</li> <li>Customization Options:    Users can fine-tune the news generation process with several parameters:<ul> <li><code>days_limit</code>: Limit the results to news published within a specific time range.</li> <li><code>article_style</code>: Specify the style of the generated news items, such as \"news article\" or \"technical blog post\" (default is \"analytical research reports\").</li> <li><code>output_language</code>: Specify the language for the output of the news items.</li> <li><code>word_count</code>: Control the number of words in the output sections (empty means automatic).</li> </ul> </li> <li>Result Output:</li> <li>The final output is a list of relevant news items, ranked by source count, each containing references to the original reporting sources.</li> </ol>"},{"location":"Flow/news/#key-benefits","title":"Key Benefits:","text":"<ul> <li>Latest News: Always retrieve the most up-to-date news from your local knowledge base.</li> <li>Duplication-Free: Duplicates are removed, so you only see unique news items.</li> <li>Source Ranking: News items are ranked by how many different sources report on them, ensuring you see the most important stories first.</li> <li>Customization: Adjust how news items are presented and filtered to meet your needs.</li> </ul>"},{"location":"Flow/news/#example-user-case","title":"Example User Case:","text":"<p>To generate a list of the most recent news on \"AI in Healthcare\" from your local KB, you can run the following command:</p> <pre><code>% leet flow -t news -q \"Can we trust ai in Healthcare\" -k \"AI in Healthcare\" -l info -p days_limit=30 -p article_style=\"news article\" -p output_language=\"en\"\n\n</code></pre>"},{"location":"Flow/news/#explanation","title":"Explanation:","text":"<ul> <li><code>t news</code>: Specifies the News workflow.</li> <li><code>l info</code>: Sets the log level to <code>info</code>, which provides essential log messages.</li> <li><code>p days_limit=30</code>: Limits the news results to items published within the last 30 days.</li> <li><code>p article_style=\"news article\"</code>: Specifies the output style of the news items as a \"news article.\"</li> <li><code>p output_language=\"en\"</code>: Outputs the results in English.</li> </ul> <p>This workflow is ideal for users who want to stay informed about a specific topic, ensuring that they only receive fresh, relevant news while avoiding duplicate reporting. </p>"},{"location":"Flow/opinions/","title":"Opinions","text":""},{"location":"Flow/opinions/#feature-opinions","title":"Feature: Opinions","text":"<p>Description:</p> <p>The Opinions workflow in LeetTools is designed to help users gather and analyze both factual information and subjective opinions on a given topic. This feature queries the web or local knowledge bases (KBs) for relevant content, processes the data to identify key facts and sentiments, and then combines them to generate a comprehensive report on the subject. It's particularly useful for obtaining a balanced view of a topic by capturing both factual data and emotional or subjective insights.</p>"},{"location":"Flow/opinions/#how-it-works","title":"How it Works:","text":"<ol> <li>Topic Query and Web Search:</li> <li>Local KB Search: If a local KB is provided (<code>docsource_uuid</code>), the query is executed within it, retrieving the top segments that match the topic.</li> <li>Web Search: If no KB is specified, a search is performed on the web using a specified search engine (e.g., Google). The results include web pages that provide relevant information on the topic.</li> <li>Crawling and Scraping:</li> <li>The workflow scrapes the top web pages or KB entries to collect relevant content for analysis.</li> <li>This content is saved to a local KB for further processing, ensuring that all scraped data is preserved for reference.</li> <li>Sentiment and Fact Extraction:</li> <li>For each scraped page, the workflow scans the content to identify sentiments (positive, negative, or neutral) and factual data points.</li> <li>The extracted facts and sentiments are stored in a database and combined to create a balanced representation of the topic.</li> <li>Dedupe and Combine:</li> <li>Duplicate sentiments and facts are removed to ensure that only unique data points are included in the final report.</li> <li>The sentiments and facts are then combined into a coherent summary.</li> <li>Report Generation:</li> <li>A final report is generated, containing both the key facts and sentiments associated with the topic. This allows users to quickly understand different perspectives on the issue.</li> <li>Customization Options:    Users can customize the behavior of the Opinions workflow with several options:<ul> <li><code>days_limit</code>: Limit search results to content published within a specific time range.</li> <li><code>excluded_sites</code>: Exclude certain websites from the search results.</li> <li><code>opinions_instruction</code>: Provide backend settings to control how sentiments and facts are extracted.</li> <li><code>summarizing_model</code>: Specify the model used to summarize the scraped articles (default is <code>gpt-4o-mini</code>).</li> <li><code>writing_model</code>: Specify the model used to generate each section of the final report (default is <code>gpt-4o-mini</code>).</li> <li><code>search_max_results</code>: Limit the maximum number of search results retrieved from the web.</li> <li><code>search_iteration</code>: Control how many pages of search results to retrieve if the maximum number of results is not reached.</li> <li><code>search_language</code>: Specify the language of the search query.</li> </ul> </li> <li>Result Output:</li> <li>The final report includes both facts and sentiments on the topic, which are clearly separated to provide users with an understanding of the topic from both an objective and emotional perspective.</li> </ol>"},{"location":"Flow/opinions/#key-benefits","title":"Key Benefits:","text":"<ul> <li>Comprehensive Analysis: Combines facts and sentiments to provide a well-rounded view of the topic.</li> <li>Customizable Workflow: Control various aspects of the search and analysis process with multiple parameters.</li> <li>Efficient Sentiment and Fact Extraction: Uses AI models to accurately identify and extract relevant data points from large volumes of text.</li> <li>Time-Sensitive Information: Filter results by time range to gather only the most relevant and up-to-date content.</li> </ul>"},{"location":"Flow/opinions/#example-user-case","title":"Example User Case:","text":"<p>To analyze the topic of \"AI in Healthcare\" and generate a report with a summary of facts and sentiments, you can run the following command:</p> <pre><code>% leet flow -t opinions -q \"AI in Healthcare\" -l info -p search_max_results=20 -p days_limit=30 -k \"AI in Healthcare\"\n\n</code></pre>"},{"location":"Flow/opinions/#explanation","title":"Explanation:","text":"<ul> <li><code>t opinions</code>: Specifies the Opinions workflow.</li> <li><code>q \"AI in Healthcare\"</code>: The topic to analyze.</li> <li><code>l info</code>: Sets the log level to <code>info</code>, showing essential log messages.</li> <li><code>p search_max_results=20</code>: Limits the search to a maximum of 20 results.</li> <li><code>p days_limit=30</code>: Filters search results to include only content from the past 30 days.</li> </ul> <p>This workflow provides an efficient way to gather and analyze both factual information and public opinions about a topic, helping users get a comprehensive understanding of the subject matter. </p>"},{"location":"Flow/search/","title":"Search","text":""},{"location":"Flow/search/#feature-search","title":"Feature: Search","text":"<p>Description:</p> <p>The Search workflow in LeetTools enables users to search for and retrieve the top segments of documents that match a given query. The workflow utilizes a combination of search techniques, including hybrid search (full-text and vector search) to return the most relevant results. It supports searching in both local knowledge bases (KB) and the web, and provides links to the original documents for easy reference.</p>"},{"location":"Flow/search/#how-it-works","title":"How it Works:","text":"<ol> <li>Query and Search Mechanism:</li> <li>Local KB Search: If a local knowledge base (KB) is provided (<code>docsource_uuid</code>), the query is executed within it using a hybrid search approach, combining full-text search with vector-based search methods like SPLADE and Vector Cosine similarity.</li> <li>Web Search: If no KB is provided, the workflow will search the web using a search engine (e.g., Google). You can specify custom search behaviors, such as excluding certain websites (<code>excluded_sites</code>) or limiting the search to specific sites (<code>target_site</code>).</li> <li>Document Pipeline:</li> <li>Search results are processed through the document pipeline, which includes conversion, chunking, and indexing of the fetched content.</li> <li>Each document is divided into smaller segments for easier matching with the query.</li> <li>Hybrid Search:</li> <li>The search results are ranked and scored based on relevance using a combination of SPLADE and Vector Cosine similarity.</li> <li>The top matching segments are returned along with their ranking score and links to the original documents.</li> <li>Customization Options:    Users can customize the search behavior with several options, including:<ul> <li><code>days_limit</code>: Limit the search results to documents within a certain time range (useful for recent content).</li> <li><code>excluded_sites</code>: Specify a comma-separated list of sites to exclude from the search results.</li> <li><code>image_search</code>: Restrict the search to image results (default is false).</li> <li><code>recursive_scrape</code>: Enable recursive scraping to gather additional content from top URLs found in the initial search results.</li> <li><code>retriever_type</code>: Choose the retriever for web searches (default is <code>google</code>).</li> <li><code>search_max_results</code>: Control the maximum number of search results to retrieve (default is 10).</li> <li><code>search_iteration</code>: Control how many times the search process should go to the next page of results if the maximum number of results is not reached.</li> <li><code>search_language</code>: Specify the language for the search query, if the search API supports it.</li> <li><code>target_site</code>: Limit the search to a specific site or domain (useful for narrowing search results).</li> </ul> </li> <li>Result Output:</li> <li>The workflow returns the top-ranked segments that match the query, including links to the original documents. This allows users to easily navigate to the full content for further reading.</li> </ol>"},{"location":"Flow/search/#key-benefits","title":"Key Benefits:","text":"<ul> <li>Relevance and Precision: Hybrid search techniques (full-text + vector search) ensure that the most relevant results are returned with high accuracy.</li> <li>Customizable Search Parameters: Fine-tune your search with various parameters, such as search language, excluded sites, and recursive scraping.</li> <li>Efficient Search: Quickly retrieves top matching segments and provides easy access to the full documents with their original links.</li> <li>Enhanced Search Logic: Uses SPLADE and Vector Cosine similarity to rank documents, ensuring that results are based on both semantic and keyword relevance.</li> </ul>"},{"location":"Flow/search/#example-user-case","title":"Example User Case:","text":"<p>To search for top segments related to \"Quantum Computing\" and exclude results from a specific website, you can run the following command:</p> <pre><code>leet flow -t search -q \"What is Quantum Computing?\" -l info -p search_max_results=15 -p search_iteration=2\n</code></pre>"},{"location":"Flow/search/#explanation","title":"Explanation:","text":"<ul> <li><code>t search</code>: Specifies the Search workflow.</li> <li><code>q \"What is Quantum Computing?\"</code>: The query to search for segments related to Quantum Computing.</li> <li><code>l info</code>: Sets the log level to <code>info</code>, showing essential log messages.</li> <li><code>p search_max_results=15</code>: Limits the search to a maximum of 15 results.</li> <li><code>p search_iteration=2</code>: Allows the search to go two pages deep for additional results.</li> </ul> <p>This command searches for the top segments related to your query and excludes results from the specified site, ensuring that the search results are highly relevant to your needs. </p>"},{"location":"LLM%20Endpoints/deepseek/","title":"Using DeepSeek API","text":"<p>We can also use any OpenAI-compatible LLM inference endpoint by setting the related  environment variable. For example, we can use the DeepSeek API by setting the following environment variables:</p> <pre><code>### to use other API providers such as DeepSeek, you can\n% export EDS_DEFAULT_OPENAI_BASE_URL=https://api.deepseek.com/v1\n% export EDS_OPENAI_API_KEY=&lt;your deepseek api key&gt;\n% export EDS_DEFAULT_OPENAI_MODEL=deepseek-chat\n# use a local embedder since DeepSeek does not provide an embedding endpoint yet\n# if the API supports OpenAI-compatible embedding endpoint, no extra settings needed\n# this dense_embedder_local_mem uses all-MiniLM-L6-v2 model as a singleton embedder\n% export EDS_DEFAULT_DENSE_EMBEDDER=dense_embedder_local_mem\n\n# Or you can put the above settings in the .env.deepseek file\n% cat .env.deepseek\nLEET_HOME=/Users/myhome/leettools\nEDS_DEFAULT_OPENAI_BASE_URL=https://api.deepseek.com/v1\nEDS_OPENAI_API_KEY=sk-0d8-mykey\nEDS_DEFAULT_OPENAI_MODEL=deepseek-chat\nEDS_DEFAULT_DENSE_EMBEDDER=dense_embedder_local_mem\n\n# Then run the command with the -e option to specify the .env file to use\n% leet flow -e .env.deepseek -t answer -q \"How does GraphRAG work?\" -k graphrag -l info\n</code></pre>"}]}